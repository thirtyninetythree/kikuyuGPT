{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLjcBvxyDzGH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets pandas huggingface_hub -q\n",
        "!pip install -U datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "I88mIqydD7Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Tokenizer From Scratch**"
      ],
      "metadata": {
        "id": "2mjeruVWEeWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way ensure the best performance and doesn't require too much compute."
      ],
      "metadata": {
        "id": "idetZ1fMEqUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, decoders, processors\n",
        "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import tempfile\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "OuMh-HVcIDVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kikuyu_tokenizer_from_scratch(\n",
        "    dataset_name=\"thirtyninetythree/kikuyu_monolingual_sentences\",  # This is a combination of Kikuyu sentences datasets available on huggingface, deduplicated and cleaned\n",
        "    text_column=\"text\",\n",
        "    vocab_size=32000,\n",
        "    model_name=\"kikuyu-tokenizer\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a BPE tokenizer from scratch optimized for Kikuyu language\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üöÄ Training Kikuyu tokenizer from scratch\")\n",
        "    print(f\"üìä Target vocab size: {vocab_size:,}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Load dataset from Hugging Face\n",
        "    print(\"Loading dataset from Hugging Face...\")\n",
        "    try:\n",
        "        dataset = load_dataset(dataset_name, split=\"train\", download_mode=\"force_redownload\",)\n",
        "        print(f\"Loaded {len(dataset):,} examples\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        print(\"Make sure your dataset is public or you're logged in\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Prepare training data\n",
        "    print(\"\\nüìù Preparing training text...\")\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n",
        "        temp_file = f.name\n",
        "\n",
        "        total_chars = 0\n",
        "        total_lines = 0\n",
        "\n",
        "        for example in dataset:\n",
        "            text = example[text_column].strip()\n",
        "            if text:  # Skip empty texts\n",
        "                f.write(text + '\\n')\n",
        "                total_chars += len(text)\n",
        "                total_lines += 1\n",
        "\n",
        "    print(f\"Training data prepared:\")\n",
        "    print(f\"   Temp file: {temp_file}\")\n",
        "    print(f\"   Lines: {total_lines:,}\")\n",
        "    print(f\"   Characters: {total_chars:,}\")\n",
        "    print(f\"   Avg chars/line: {total_chars/total_lines:.1f}\")\n",
        "\n",
        "    # Step 3: Initialize tokenizer\n",
        "    print(\"\\nInitializing BPE tokenizer...\")\n",
        "\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "    # Normalizer: Critical for Kikuyu diacritics (≈©, ƒ©, etc.)\n",
        "    tokenizer.normalizer = normalizers.Sequence([\n",
        "        normalizers.NFKC(),  # Unicode normalization for diacritics\n",
        "        # NOT lowercasing - preserve Kikuyu capitalization\n",
        "    ])\n",
        "\n",
        "    # Pre-tokenizer: How to split text before BPE\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "        pre_tokenizers.WhitespaceSplit(),  # Split on whitespace\n",
        "        pre_tokenizers.Punctuation(),     # Separate punctuation\n",
        "    ])\n",
        "\n",
        "    # Decoder: COnverts tokens back to text\n",
        "    tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n",
        "\n",
        "    # Step 4: Set up trainer with Kikuyu-specific special tokens\n",
        "    print(\"‚öôÔ∏è Setting up BPE trainer...\")\n",
        "\n",
        "    special_tokens = [\n",
        "        \"<pad>\",    # Padding token\n",
        "        \"<unk>\",    # Unknown token\n",
        "        \"<s>\",      # Start of sequence (Llama compatibility)\n",
        "        \"</s>\",     # End of sequence (Llama compatibility)\n",
        "        \"[INST]\",   # Instruction start (for future chat models)\n",
        "        \"[/INST]\",  # Instruction end\n",
        "        \"<<SYS>>\",  # System message start\n",
        "        \"<</SYS>>\", # System message end\n",
        "    ]\n",
        "\n",
        "    trainer = trainers.BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=special_tokens,\n",
        "        min_frequency=2,  # Don't include very rare tokens\n",
        "        continuing_subword_prefix=\"\",  # BPE suffix\n",
        "        end_of_word_suffix=\"</w>\",     # End of word marker\n",
        "        show_progress=True,\n",
        "    )\n",
        "\n",
        "    # Step 5: Train the tokenizer\n",
        "    print(f\"\\nTraining tokenizer on {total_lines:,} lines...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    tokenizer.train([temp_file], trainer)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"raining completed in {training_time:.1f} seconds\")\n",
        "\n",
        "    # Step 6: Add post-processor for proper token handling\n",
        "    tokenizer.post_processor = processors.TemplateProcessing(\n",
        "        single=\"<s> $A </s>\",\n",
        "        pair=\"<s> $A </s> <s> $B </s>\",\n",
        "        special_tokens=[\n",
        "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Step 7: Save raw tokenizer\n",
        "    tokenizer_file = f\"{model_name}.json\"\n",
        "    tokenizer.save(tokenizer_file)\n",
        "    print(f\"Raw tokenizer saved: {tokenizer_file}\")\n",
        "\n",
        "    # Step 8: Create Hugging Face compatible tokenizer\n",
        "    print(\"\\nCreating Hugging Face compatible tokenizer...\")\n",
        "\n",
        "    hf_tokenizer = PreTrainedTokenizerFast(\n",
        "        tokenizer_object=tokenizer,\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        bos_token=\"<s>\",\n",
        "        eos_token=\"</s>\",\n",
        "        additional_special_tokens=[\"[INST]\", \"[/INST]\", \"<<SYS>>\", \"<</SYS>>\"],\n",
        "        clean_up_tokenization_spaces=True\n",
        "    )\n",
        "\n",
        "    # Save HuggingFace tokenizer\n",
        "    hf_tokenizer.save_pretrained(model_name)\n",
        "    print(f\"üíæ HuggingFace tokenizer saved: {model_name}/\")\n",
        "\n",
        "    # Step 9: Test the tokenizer\n",
        "    print(\"\\nüß™ Testing tokenizer...\")\n",
        "    test_kikuyu_tokenizer(hf_tokenizer)\n",
        "\n",
        "    # Cleanup\n",
        "    os.unlink(temp_file)\n",
        "\n",
        "    print(f\"\\nüéâ SUCCESS! Kikuyu tokenizer trained from scratch\")\n",
        "    print(f\"üìÅ Files created:\")\n",
        "    print(f\"   ‚Ä¢ {tokenizer_file} (raw tokenizer)\")\n",
        "    print(f\"   ‚Ä¢ {model_name}/ (HuggingFace format)\")\n",
        "\n",
        "    return hf_tokenizer\n",
        "\n",
        "def test_kikuyu_tokenizer(tokenizer):\n",
        "    \"\"\"Test the tokenizer with Kikuyu text samples\"\"\"\n",
        "\n",
        "    test_texts = [\n",
        "        \"Wanjiku nƒ© m≈©r≈©ithia wa k≈©heshimu.\",\n",
        "        \"Kamau anapenda g≈©thoma vitabu vingi.\",\n",
        "        \"M≈©nd≈© ≈©≈© nƒ© m≈©heshimiwa m≈©no g≈©k≈©.\",\n",
        "        \"And≈© aya nƒ© marafiki ma ma.\",\n",
        "        \"Kƒ©nd≈© kƒ©u nƒ© kƒ©ega m≈©no k≈©rƒ© and≈© othe.\",\n",
        "        \"Nyambura aracokia m≈©ciƒ© k≈©uma wƒ©ra-inƒ©.\",\n",
        "    ]\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"KIKUYU TOKENIZER TEST\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    total_chars = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for i, text in enumerate(test_texts, 1):\n",
        "        # Tokenize\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "        # Stats\n",
        "        total_chars += len(text)\n",
        "        total_tokens += len(tokens)\n",
        "\n",
        "        print(f\"\\n{i}. Text: {text}\")\n",
        "        print(f\"   Tokens ({len(tokens)}): {tokens}\")\n",
        "        print(f\"   IDs: {token_ids}\")\n",
        "\n",
        "        # Test decoding\n",
        "        decoded = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "        if decoded.strip() == text.strip():\n",
        "            print(f\"   ‚úÖ Decoding: Perfect\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Decoding: '{decoded}' (slight difference)\")\n",
        "\n",
        "    # Overall stats\n",
        "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "    print(f\"\\nüìä OVERALL PERFORMANCE:\")\n",
        "    print(f\"   Characters: {total_chars}\")\n",
        "    print(f\"   Tokens: {total_tokens}\")\n",
        "    print(f\"   Compression: {compression_ratio:.2f} chars/token\")\n",
        "    print(f\"   Vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "    return compression_ratio"
      ],
      "metadata": {
        "id": "70iCQroFHX3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"üöÄ Kikuyu Tokenizer Training from Scratch\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Configure your dataset\n",
        "    DATASET_NAME = \"thirtyninetythree/kikuyu_monolingual_sentences\"\n",
        "    TEXT_COLUMN = \"text\"\n",
        "\n",
        "    print(f\"üìä Dataset: {DATASET_NAME}\")\n",
        "    print(f\"üìù Text column: {TEXT_COLUMN}\")\n",
        "    print(f\"üéØ Target: 32,000 vocabulary size\")\n",
        "\n",
        "    # Train tokenizer\n",
        "    custom_tokenizer = train_kikuyu_tokenizer_from_scratch(\n",
        "        dataset_name=DATASET_NAME,\n",
        "        text_column=TEXT_COLUMN,\n",
        "        vocab_size=32000,\n",
        "        model_name=\"kikuyu-bpe-tokenizer\"\n",
        "    )\n",
        "\n",
        "    custom_tokenizer.save_pretrained(\"kikuyu-bpe-tokenizer\")\n",
        "    custom_tokenizer.push_to_hub(\"kikuyu-bpe-tokenizer\")\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Mlg5G7I_FYqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the tokenizer"
      ],
      "metadata": {
        "id": "TXlhZfaCFyBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast\n",
        "import torch\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"thirtyninetythree/kikuyu-bpe-tokenizer\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Example Kikuyu inpu text\n",
        "prompt = \"Niwega muno\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
        "\n",
        "output_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.8,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "c2hRuRqEFzh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **LoRa Fine tune**\n",
        "\n"
      ],
      "metadata": {
        "id": "3J8DHTldGHyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model # type: ignore\n",
        "import os\n",
        "\n",
        "\n",
        "HUB_REPO_NAME = \"thirtyninetythree/TinyLlama-1.1B-Kikuyu-LoRA\"\n",
        "TOKENIZER_PATH = \"thirtyninetythree/kikuyu-bpe-tokenizer\"\n",
        "DATASET_PATH = \"thirtyninetythree/kikuyu_monolingual_sentences\"\n",
        "BASE_MODEL_PATH = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# --- 1. Load Tokenizer and Base Model ---\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
        "\n",
        "# Add padding token if it doesn't exist (common for Llama-like models for training)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "    print(f\"Added pad_token: '{tokenizer.pad_token}' with ID {tokenizer.pad_token_id}\")\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# 2. Resize Token Embeddings (Crucial for new vocabulary from your tokenizer)\n",
        "print(f\"Original model embedding size: {model.get_input_embeddings().weight.shape[0]}\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print(f\"Resized model embedding size: {model.get_input_embeddings().weight.shape[0]}\")\n",
        "\n",
        "# --- 3. Configure LoRA ---\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# --- 4. Get the PEFT (LoRA) Model ---\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 5. Prepare Dataset ---\n",
        "print(\"\\nLoading and preparing dataset...\")\n",
        "dataset = load_dataset(DATASET_PATH, split=\"train\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=[\"text\"],\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# --- 6. Set up Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_kikuyu_llama_lora\", # Temporary output directory for trainer\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs_lora\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# --- 7. Start Fine-tuning with Trainer ---\n",
        "print(\"\\nStarting LoRA fine-tuning...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"\\nLoRA Fine-tuning complete.\")\n",
        "\n",
        "# --- 8. Push LoRA Adapters and Tokenizer to Hugging Face Hub ---\n",
        "print(f\"\\nPushing LoRA adapters and tokenizer to Hugging Face Hub at: {HUB_REPO_NAME}\")\n",
        "try:\n",
        "   trainer.model.push_to_hub(HUB_REPO_NAME)\n",
        "   tokenizer.push_to_hub(HUB_REPO_NAME)\n",
        "   print(\"Successfully pushed model adapters and tokenizer to Hugging Face Hub!\")\n",
        "   print(f\"You can view your model at: https://huggingface.co/{HUB_REPO_NAME}\")\n",
        "except Exception as e:\n",
        "   print(f\"Failed to push to Hugging Face Hub: {e}\")\n",
        "   print(\"Please ensure you are logged in: `huggingface-cli login`\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AyqY3Ql5GHLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to device (trainer.train() should have done this, but explicit for clarity)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "   tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "prompts = [\n",
        "   \"Niwega muno\",\n",
        "   \"Mathomo ma Kikuyu ni\",\n",
        "   \"Mwana wa ndege\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "   inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
        "   output_ids = model.generate(\n",
        "       **inputs,\n",
        "       max_new_tokens=50,\n",
        "       do_sample=True,\n",
        "       top_p=0.95,\n",
        "       temperature=0.8,\n",
        "       eos_token_id=tokenizer.eos_token_id,\n",
        "       pad_token_id=tokenizer.pad_token_id,\n",
        "   )\n",
        "   output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "   print(f\"\\n--- Prediction {i+1} ---\")\n",
        "   print(f\"Prompt: {prompt}\")\n",
        "   print(f\"Generated Output: {output_text}\")\n"
      ],
      "metadata": {
        "id": "hZcoS8o-IfY1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}